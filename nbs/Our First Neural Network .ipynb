{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Our First Neural Network .ipynb","provenance":[],"collapsed_sections":["7u8l3TjF94Yt"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"2jpOe_1194Xk","colab_type":"text"},"source":["# Lets build out first Deep Neural Network"]},{"cell_type":"markdown","metadata":{"id":"EFCIl7Qn94Xl","colab_type":"text"},"source":["All dependencies for this notebook is listed in the requirements.txt file. One parent above the nbs directory. This list will keep changing as we add to it so be sure to rerun this line after every git pull"]},{"cell_type":"code","metadata":{"id":"EkNcj5Dy-Bld","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O_VbeYWk-VXV","colab_type":"code","colab":{}},"source":["%cd gdrive/My Drive/UCI/cs274p"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MHDuZ96A94Xm","colab_type":"code","colab":{}},"source":["!pip install -r requirements.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYsmga0r94Xq","colab_type":"text"},"source":["Lets declare our imports"]},{"cell_type":"code","metadata":{"id":"SPt2n3EJ94Xr","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","from torch import nn\n","import math\n","from pprint import pprint\n","from tqdm.notebook import tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C76sD6t-94Xu","colab_type":"code","colab":{}},"source":["! pip install -q kaggle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdmaadJ094Xx","colab_type":"code","colab":{}},"source":["from google.colab import files"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hE4fvse094X0","colab_type":"code","colab":{}},"source":["files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7GVHk6JX94X3","colab_type":"code","colab":{}},"source":["! mkdir ~/.kaggle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-R8tCBc94X7","colab_type":"code","colab":{}},"source":["! cp kaggle.json ~/.kaggle/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCJROWVE94X-","colab_type":"code","colab":{}},"source":["! chmod 600 ~/.kaggle/kaggle.json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyy4yB2F94YB","colab_type":"code","colab":{}},"source":["! kaggle datasets list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDkldQFG94YD","colab_type":"code","colab":{}},"source":["! kaggle datasets download -d ronitf/heart-disease-uci"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82Pn3kkt94YH","colab_type":"code","colab":{}},"source":["class MyFirstNeuralNetwork(torch.nn.Module):\n","    def __init__(self, in_size=2, out_size=2, hidden_size=3):\n","\n","        super(MyFirstNeuralNetwork, self).__init__()\n","\n","        # Set the dimensionality of the network\n","        self.input_size = in_size\n","        self.output_size = out_size\n","        self.hidden_size = hidden_size\n","        self.learning_rate = 0.5\n","\n","        # Initialize our weights\n","        self._init_weights()\n","\n","    '''\n","    Initialize the weights\n","    '''\n","    def _init_weights(self):\n","        # Create an input tensor of shape (in_size, hidden_size)\n","        self.W_Input = torch.randn(self.input_size, self.hidden_size) / math.sqrt(self.input_size)\n","        # Create an output tensor of shape (3, 1)\n","        self.W_Output = torch.randn(self.hidden_size, self.output_size) / math.sqrt(self.hidden_size)\n","\n","        self.bias_in = torch.randn(self.input_size)\n","        self.bias_hidden = torch.randn(self.hidden_size)\n","        \n","    '''\n","    Create the forward pass\n","    '''\n","    def forward(self, inputs):\n","        # Lets get the element wise dot product\n","        self.z = torch.matmul(inputs, self.W_Input) + self.bias_in\n","        # We call the activation\n","        self.state = self._activation(self.z)\n","        # Pass it through the hidden layer\n","        self.z_hidden = torch.matmul(self.state, self.W_Output) + self.hidden_size\n","        # Finally activate the output\n","        output = self._activation(self.z_hidden)\n","        # Return the output\n","        return output\n","\n","    '''\n","    Backpropagation algorithm implemented\n","    '''\n","    def backward(self, inputs, labels, output):\n","        # What is the error in output\n","        self.loss = labels - output\n","        # What is the delta loss based on the derivative\n","        self.loss_delta = self.learning_rate * self.loss * self._derivative(output)\n","        # Get the loss for the existing output weight\n","        self.z_loss = torch.matmul(self.loss_delta, torch.t(self.W_Output))\n","        # Compute the delta like before\n","        self.z_loss_delta = self.learning_rate * self.z_loss * self._derivative(self.state)\n","        # Finally propogate this to our existing weight tensors to update\n","        # the gradient loss\n","        self.W_Input += torch.matmul(torch.t(inputs), self.z_loss_delta)\n","        self.W_Output += torch.matmul(torch.t(self.state), self.loss_delta)\n","\n","    '''\n","    Here we train the network\n","    '''\n","    def train(self, inputs, labels):\n","        # First we do the foward pass\n","        outputs = self.forward(inputs)\n","        # Then we do the backwards pass\n","        self.backward(inputs, labels, outputs)\n","\n","    '''\n","    Here we perform inference\n","    '''\n","    def predict(self, inputs):\n","        pass\n","\n","    '''\n","    Here we save the model\n","    '''\n","    def save(self, out_path):\n","        self.save(out_path)\n","    \n","    '''\n","    Our non-linear activation function\n","    '''\n","    def _activation(self, s):\n","        # Lets use sigmoid\n","        return 1 / (1 + torch.exp(-s))\n","\n","    '''\n","    Our derivative function used for backpropagation\n","    Usually the sigmoid prime\n","    '''\n","    def _derivative(self, s):\n","        # derivative of sigmoid\n","        return s * (1 - s)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"obdcUZcY94YL","colab_type":"code","colab":{}},"source":["import pandas as pd\n","df = pd.read_csv('data/heart.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t1z0p34B94YO","colab_type":"code","colab":{}},"source":["df.head(20)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJ2Z7u2T94YQ","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import accuracy_score\n","sc = MinMaxScaler((-1, 1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YiZLzopU94YT","colab_type":"text"},"source":["Lets split out dataset between inputs and target"]},{"cell_type":"code","metadata":{"id":"shVcxAkp94YU","colab_type":"code","colab":{}},"source":["df.shape\n","y = df['target']\n","X = df.drop('target', axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsfHEQJ_94YW","colab_type":"text"},"source":["Lets create a test and train split"]},{"cell_type":"code","metadata":{"id":"B_1G1Rxy94YX","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2fDmfeEc94Ya","colab_type":"text"},"source":["Here we transform features by scaling each feature to a given range.\n","This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one."]},{"cell_type":"code","metadata":{"id":"Eq5PC0p694Yb","colab_type":"code","colab":{}},"source":["X_train = sc.fit_transform(X_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0zpTKWt94Ye","colab_type":"code","colab":{}},"source":["X_train = torch.tensor(X_train, dtype=torch.float)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwU9gInv94Yg","colab_type":"code","colab":{}},"source":["y_train = torch.tensor((y_train.values,))\n","y_train = y_train.transpose(0,1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Y4S9Uw_94Yl","colab_type":"code","colab":{}},"source":["print(X_train.shape, y_train.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6iH99lj94Yn","colab_type":"text"},"source":["Now we instantiate our neural network"]},{"cell_type":"code","metadata":{"id":"tR3jzq2G94Yo","colab_type":"code","colab":{}},"source":["nn = MyFirstNeuralNetwork(in_size=X_train.shape[1], out_size=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUL4AViV94Yq","colab_type":"text"},"source":["We train our neural network with 1000 epochs (training loops) and we measure the loss"]},{"cell_type":"code","metadata":{"id":"iWMP2eNm94Yr","colab_type":"code","colab":{}},"source":["for i in tqdm(range(50)):\n","    outputs = nn(X_train)\n","    loss = torch.mean((y_train - outputs)**2).detach().item()\n","    tqdm.write(\"Loss: {}\".format(loss))\n","    nn.train(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7u8l3TjF94Yt","colab_type":"text"},"source":["# Excercises"]},{"cell_type":"markdown","metadata":{"id":"rtrpAPez94Yu","colab_type":"text"},"source":["1. Try to initialize the weights with something better. Hint (Xavier Initialization)\n","2. Add a bias to the forward pass. Recall the affine transform is (inputs . weights) + bias\n","3. We are missing a learning rate to the backwards pass. See if you can add that in"]},{"cell_type":"markdown","metadata":{"id":"iZDjlLnr94Yv","colab_type":"text"},"source":["# How would we rewrite this code using PyTorch built-in methods"]},{"cell_type":"markdown","metadata":{"id":"Nps6aj9z94Yv","colab_type":"text"},"source":["PyTorch gives us most of this functionality out of the box. First we can flag all Tensors to use Autograd. You can read more about autograd here: https://pytorch.org/docs/stable/autograd.html"]},{"cell_type":"code","metadata":{"id":"S1Ggg9oP94Yw","colab_type":"code","colab":{}},"source":["X = df.drop('thalach', axis=1)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n","# Populate the best\n","X_train = torch.tensor(sc.fit_transform(X_train), dtype=torch.float, requires_grad=True)\n","X_test = torch.tensor(sc.transform(X_test), dtype=torch.float)\n","\n","y_train = torch.tensor(y_train.values)\n","y_test = torch.tensor(y_test.values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V7j3SmwP94Yy","colab_type":"text"},"source":["This is the first way using the torch.nn.Sequential. In the Sequential model modules will be added to it in the order they are passed in the constructor. This is a quick way to write a small neural network"]},{"cell_type":"code","metadata":{"id":"kXdSH1bA94Yz","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","from collections import OrderedDict\n","\n","model = torch.nn.Sequential(OrderedDict([\n","    ('fc1', nn.Linear(13, 100)),\n","    ('relu1', nn.ReLU()),\n","    ('fc2', nn.Linear(100, 100)),\n","    ('relu2', nn.ReLU()),\n","    ('fc3', nn.Linear(100, 2)),\n","    ('sigmoid', nn.Sigmoid())\n","]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wpOpP-F_8lb","colab_type":"text"},"source":["What does the architecture of this model look like"]},{"cell_type":"code","metadata":{"id":"j5Gst-XK_v4F","colab_type":"code","colab":{}},"source":["print(model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-ihiagV94Y1","colab_type":"text"},"source":["Lets register an optimizer and an objective function"]},{"cell_type":"code","metadata":{"id":"5sUg844j94Y2","colab_type":"code","colab":{}},"source":["optimizer = torch.optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss()\n","losses = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxKib_mq94Y4","colab_type":"code","colab":{}},"source":["for epoch in tqdm(range(1000)):\n","    optimizer.zero_grad()\n","    outputs = model(X_train)\n","    loss = criterion(outputs, y_train)\n","    loss.backward()\n","    optimizer.step()\n","    losses.append(loss.item())\n","    print(\"Epoch {}, Loss: {}\".format(epoch, loss.item()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOEp87X794Y7","colab_type":"code","colab":{}},"source":["prediction = model(X_test)\n","_, preds_y = torch.max(prediction, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cov66vFd94Y9","colab_type":"code","colab":{}},"source":["accuracy_score(y_test, preds_y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hDMPJ8PQ94Y_","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.plot(losses, label=\"Loss Curve\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBOSzAdB94ZE","colab_type":"code","colab":{}},"source":["losses_no_back = []\n","for epoch in tqdm(range(1000)):\n","    optimizer.zero_grad()\n","    outputs = model(X_train)\n","    loss = criterion(outputs, y_train)\n","    optimizer.step()\n","    losses_no_back.append(loss.item())\n","    print(\"Epoch {}, Loss: {}\".format(epoch, loss.item()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oNj8WlIV94ZH","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.plot(losses_no_back, label=\"Loss Curve\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DeUwKUK094ZJ","colab_type":"text"},"source":["# A Better way to Torch with Py"]},{"cell_type":"markdown","metadata":{"id":"N-IwO-Ap94ZK","colab_type":"text"},"source":["We looked at the torch.nn.Module before. Inherting from this class gives us:\n","\n","1. More flexibility on how we build our layers\n","2. Encapsulate our logic into one object\n","3. Easily swap out optimization functions\n","4. Easilty swap out cost functions"]},{"cell_type":"code","metadata":{"id":"-Mu9Bzl-94ZK","colab_type":"code","colab":{}},"source":["class DeepNeuralNetwork(nn.Module):\n","    def __init__(self, in_size, out_size, hidden_size, layer_depth=4, activation=nn.ReLU):\n","        super(DeepNeuralNetwork, self).__init__()\n","        \n","        self.activation = activation()\n","        self.in_size = in_size\n","        self.out_size = out_size\n","        \n","        self.fc1 = nn.Linear(in_size, hidden_size)\n","        self.fcn = nn.ModuleDict({})\n","        \n","        for l in range(layer_depth):\n","            name = 'fc'+str(1+l)\n","            self.fcn[name] = nn.Linear(hidden_size, hidden_size)\n","        \n","        self.out = nn.Linear(hidden_size, out_size)\n","        \n","        self.optimizer = torch.optim.Adam(self.parameters())\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.loss_tracker = []\n","    \n","    def add_layer(index, layer):\n","        pass\n","    \n","    def forward(self, x):\n","        x = self.activation(self.fc1(x))\n","        for k, l in self.fcn.items():\n","            x = self.activation(x)\n","        x = self.out(x)\n","        return x\n","    \n","    def train(self, inputs, labels, test_inputs=None, test_labels=None, epochs=10) -> None:\n","        for epoch in tqdm(range(epochs)):\n","            self.optimizer.zero_grad()\n","            outputs = self(X_train)\n","            loss = self.criterion(outputs, y_train)\n","            loss.backward()\n","            self.optimizer.step()\n","            self.loss_tracker.append(loss.item())\n","            acc = self.accuracy(test_inputs, test_labels)\n","            tqdm.write(\"Epoch {}, Loss: {} Acc: {}\".format(epoch, loss.item(), acc))\n","\n","    def accuracy(self, test_inputs, test_labels):\n","        _, preds_y = torch.max(self(test_inputs), 1)\n","        return accuracy_score(test_labels, preds_y)\n","    \n","    def show_loss(self):\n","        import matplotlib.pyplot as plt\n","        plt.plot(self.loss_tracker, label=\"Loss Curve\")\n","        plt.legend()\n","        plt.show()\n","        \n","    def predict(self, inputs):\n","        \"\"\"\n","        Sets the model to evaluation/inference mode, disabling dropout and\n","        gradient calculation.\n","        \"\"\"\n","        self.eval()\n","        return self(inputs)\n","    \n","    def summary(self):\n","        from torchsummary import summary\n","        summary(self, (1, 1, self.in_size)) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldB5Erj194ZN","colab_type":"text"},"source":["We create a new instance of the Module we just created"]},{"cell_type":"code","metadata":{"id":"eW4ABg-w94ZO","colab_type":"code","colab":{}},"source":["model = DeepNeuralNetwork(13, 2, 100)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nG-q73mB94ZV","colab_type":"text"},"source":["Lets look at what the model summary or architecture looks like. \n","We will go into much more depth when look at TensorboardX"]},{"cell_type":"code","metadata":{"id":"xq1BRu-F94ZW","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1EnOqZcw94ZY","colab_type":"text"},"source":["Lets go ahead and train the model"]},{"cell_type":"code","metadata":{"id":"kL3cxgI894ZZ","colab_type":"code","colab":{}},"source":["model.train(X_train, y_train, X_test, y_test, epochs=1000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uzVsUSiK94Zc","colab_type":"text"},"source":["Lets get the model accuracy"]},{"cell_type":"code","metadata":{"id":"BsBY3eN094Zc","colab_type":"code","colab":{}},"source":["model.accuracy(X_test, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5oCcMWjz94Zf","colab_type":"text"},"source":["What does the loss curve look like"]},{"cell_type":"code","metadata":{"id":"3cNCOLCg94Zg","colab_type":"code","colab":{}},"source":["model.show_loss()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IxlSsLjG94Zj","colab_type":"text"},"source":["Lets print out the model summary one more time to see if anything has changed"]},{"cell_type":"code","metadata":{"id":"m9vahSeq94Zj","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SojMJrrd94Zm","colab_type":"text"},"source":["The model architecture we built above is sufficient for many problems. \n","But lets look at a SOTA model and see how complex it can become"]},{"cell_type":"code","metadata":{"id":"PJQ7NC8n94Zm","colab_type":"code","colab":{}},"source":["from PIL import Image\n","from IPython.display import display\n","img = Image.open('images/inception_v4.jpeg')\n","display(img)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WW5xfMS894Zp","colab_type":"text"},"source":["# Excercises"]},{"cell_type":"markdown","metadata":{"id":"oswqES2m94Zp","colab_type":"text"},"source":["1. Try to implement early stopping\n","2. Implement a different activation function. What worked, what didn't\n","3. Implement a different loss function. What worked and what didn't\n","4. Introduce a different dataset and see if you can use the above model to build an accurate model"]},{"cell_type":"markdown","metadata":{"id":"IqsXq2tY94Zq","colab_type":"text"},"source":["# References"]},{"cell_type":"markdown","metadata":{"id":"cT12FzRU94Zr","colab_type":"text"},"source":["https://arxiv.org/pdf/1602.07261.pdf"]},{"cell_type":"markdown","metadata":{"id":"Y95ijBMf94Zr","colab_type":"text"},"source":["https://pytorch.org/docs/stable/nn.html"]}]}